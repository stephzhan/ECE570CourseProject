{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stephzhan/ECE570CourseProject/blob/main/ECE570_Course_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "BK1f9-E1A8KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcSsOfvevAwz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "from typing import List, Dict\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFAeeKn3vCw-"
      },
      "outputs": [],
      "source": [
        "class LoraLayer(nn.Module):\n",
        "    def __init__(self, rank, in_dim, out_dim, alpha, dropout, merge_weights):\n",
        "        super().__init__()\n",
        "\n",
        "        self.alpha = alpha\n",
        "        if rank > 0:\n",
        "            self.A = torch.nn.Parameter(torch.empty((rank, in_dim)))\n",
        "            self.B = torch.nn.Parameter(torch.empty((out_dim, rank)))\n",
        "            self.scaling = self.alpha / rank\n",
        "\n",
        "        if dropout > 0.:\n",
        "            self.dropout = nn.Dropout(p=dropout)\n",
        "        else:\n",
        "            self.dropout = lambda x: x\n",
        "\n",
        "        self.merged = False\n",
        "        self.merge_weights = merge_weights\n",
        "\n",
        "        self.reset_parameters()\n",
        "        if device:\n",
        "            self.to(device)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.scaling * (self.dropout(x) @ self.A.T @ self.B.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS3ZfR_mhb0D"
      },
      "outputs": [],
      "source": [
        "class LoraLinear(nn.Module):\n",
        "    def __init__(self, layer, rank, alpha, dropout, merge_weights):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = layer\n",
        "        self.lora = LoraLayer(rank, self.linear.in_features, self.linear.out_features, alpha, dropout, merge_weights)\n",
        "        self.linear.weight.requires_grad = False\n",
        "        self.lora.reset_parameters()\n",
        "        self.linear.reset_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x) + self.lora(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoraConv(nn.Module):\n",
        "    def __init__(self, layer, rank, alpha, dropout, merge_weights):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = layer\n",
        "        k_size = self.conv.kernel_size\n",
        "        self.lora = LoraLayer(rank * k_size, self.conv.in_channels * k_size, self.conv.out_channels//self.conv.groups * k_size, alpha, dropout, merge_weights)\n",
        "        self.conv.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "          return self.conv._conv_forward(x, self.conv.weight + (self.lora_B @ self.lora_A).view(self.conv.weight.shape) * self.scaling, self.conv.bias)"
      ],
      "metadata": {
        "id": "i3xE8Ms_-Syg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoraEmbedding(nn.Module):\n",
        "  def __init__(self, layer, rank, alpha, dropout, merge_weights):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = layer\n",
        "        self.lora = LoraLayer(rank, layer.num_embeddings, layer.embedding_dim, alpha, dropout, merge_weights)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "  def forward(self, x):\n",
        "        pass"
      ],
      "metadata": {
        "id": "w9NmdQXMk1xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoraMergedLinear(nn.Module):\n",
        "  def __init__(self, layer, rank, alpha, dropout, merge_weights):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = layer\n",
        "        self.lora = LoraLayer(rank, layer.num_embeddings, layer.embedding_dim, alpha, dropout, merge_weights)\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "  def forward(self, x):\n",
        "        pass"
      ],
      "metadata": {
        "id": "yvBl_Fsf3KjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9nb94nLheLJ"
      },
      "outputs": [],
      "source": [
        "## CODE REFERENCED FROM ECE570 ASSIGNMENT 3 ##\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          loss_fn: nn.modules.loss._Loss,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          train_loader: torch.utils.data.DataLoader,\n",
        "          epoch: int=0)-> List:\n",
        "    # ----------- <Your code> ---------------\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    train_counter = []\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "      images, targets = images.to(device), targets.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = model(images)\n",
        "      loss = loss_fn(output, targets)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_losses.append(loss.item())\n",
        "\n",
        "      if batch_idx % 100 == 0:\n",
        "        train_counter.append(\n",
        "        (batch_idx*len(images)) + ((epoch-1)*len(train_loader.dataset)))\n",
        "        #print(f'Epoch {epoch}: [{batch_idx*len(images)}/{len(train_loader.dataset)}] Loss: {loss.item():.3f}')\n",
        "      torch.cuda.empty_cache()\n",
        "    # ----------- <End Your code> ---------------\n",
        "    assert len(train_losses) == len(train_loader)\n",
        "    return train_losses\n",
        "\n",
        "def test(model: nn.Module,\n",
        "         loss_fn: nn.modules.loss._Loss,\n",
        "         test_loader: torch.utils.data.DataLoader,\n",
        "         epoch: int=0)-> Dict:\n",
        "    # ----------- <Your code> ---------------\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    test_stat = dict()\n",
        "    test_losses = []\n",
        "    test_counter = []\n",
        "    total_num = 0\n",
        "    pred_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for images, targets in test_loader:\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "        output = model(images)\n",
        "        test_loss += F.nll_loss(output, targets, reduction='sum').item()\n",
        "        pred = output.data.argmax(1) # we get the estimate of our result by look at the largest class value\n",
        "        correct += pred.eq(targets.data.view_as(pred)).sum() # sum up the corrected samples\n",
        "        pred_list.extend(list(pred))\n",
        "        total_num = total_num + 1\n",
        "\n",
        "      test_loss /= len(test_loader.dataset)\n",
        "      test_losses.append(test_loss)\n",
        "      test_counter.append(len(test_loader.dataset)*epoch)\n",
        "\n",
        "    test_stat['loss'] = test_loss\n",
        "    test_stat['accuracy'] = correct / len(test_loader.dataset)\n",
        "    test_stat['prediction'] = torch.Tensor(pred_list).to(torch.long)\n",
        "\n",
        "    print(f\"Test result on epoch {epoch}: Acc: {100*test_stat['accuracy']:.3f}%\")\n",
        "\n",
        "    # ----------- <Your code> ---------------\n",
        "    # dictionary should include loss, accuracy and prediction\n",
        "    assert \"loss\" and \"accuracy\" and \"prediction\" in test_stat.keys()\n",
        "    # \"prediction\" value should be a 1D tensor\n",
        "    assert len(test_stat[\"prediction\"]) == len(test_loader.dataset)\n",
        "    assert isinstance(test_stat[\"prediction\"], torch.Tensor)\n",
        "    return test_stat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqMTT_OPiukA",
        "outputId": "d462539c-221a-4be2-ec2a-8570900dc8a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "## CODE TAKEN FROM ECE570 ASSIGNMENT 3 ##\n",
        "train_transform = torchvision.transforms.Compose([torchvision.transforms.RandomResizedCrop(224),\n",
        "                                                  torchvision.transforms.RandomHorizontalFlip(),\n",
        "                                                  torchvision.transforms.ToTensor(),\n",
        "                                                  torchvision.transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))])\n",
        "\n",
        "test_transform = torchvision.transforms.Compose([torchvision.transforms.Resize(256),\n",
        "                                                 transforms.CenterCrop(224),\n",
        "                                                  torchvision.transforms.ToTensor(),\n",
        "                                                  torchvision.transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10('data', train=True, download=True, transform=train_transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10('data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "batch_size_train, batch_size_test = 256, 512\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## CODE REFERENCED FROM ECE570 ASSIGNMENT 3 ##\n",
        "rank = [2, 4, 6, 8]\n",
        "max_epoch = 8\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in rank:\n",
        "  start = time.time()\n",
        "  resnet18_LORA = models.resnet18(pretrained=True)\n",
        "  resnet18_LORA.to(device)\n",
        "  resnet18_LORA.fc = LoraLinear(layer=resnet18_LORA.fc, rank=i, alpha=i, dropout=0.5, merge_weights=False)\n",
        "  optimizer = optim.SGD(resnet18_LORA.parameters(), lr=0.1, momentum=0.8)\n",
        "  for epoch in range(max_epoch):\n",
        "    train(resnet18_LORA, criterion, optimizer, train_loader, epoch)\n",
        "    test(resnet18_LORA, criterion, test_loader, epoch)\n",
        "  end = time.time()\n",
        "  print(f'Finished Training after {end-start} s ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXCz2Z7NOtWs",
        "outputId": "2428a864-3f9a-4378-c6dc-80dc97f2d8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test result on epoch 0: Acc: 20.330%\n",
            "Test result on epoch 1: Acc: 48.970%\n",
            "Test result on epoch 2: Acc: 75.010%\n",
            "Test result on epoch 3: Acc: 80.500%\n",
            "Test result on epoch 4: Acc: 84.220%\n",
            "Test result on epoch 5: Acc: 85.800%\n",
            "Test result on epoch 6: Acc: 86.870%\n",
            "Test result on epoch 7: Acc: 90.660%\n",
            "Finished Training after 907.1683824062347 s \n",
            "Test result on epoch 0: Acc: 81.890%\n",
            "Test result on epoch 1: Acc: 87.590%\n",
            "Test result on epoch 2: Acc: 89.630%\n",
            "Test result on epoch 3: Acc: 91.440%\n",
            "Test result on epoch 4: Acc: 91.330%\n",
            "Test result on epoch 5: Acc: 91.140%\n",
            "Test result on epoch 6: Acc: 91.380%\n",
            "Test result on epoch 7: Acc: 92.710%\n",
            "Finished Training after 902.1033148765564 s \n",
            "Test result on epoch 0: Acc: 30.860%\n",
            "Test result on epoch 1: Acc: 42.360%\n",
            "Test result on epoch 2: Acc: 63.570%\n",
            "Test result on epoch 3: Acc: 75.820%\n",
            "Test result on epoch 4: Acc: 78.920%\n",
            "Test result on epoch 5: Acc: 80.470%\n",
            "Test result on epoch 6: Acc: 85.200%\n",
            "Test result on epoch 7: Acc: 86.520%\n",
            "Finished Training after 897.720733165741 s \n",
            "Test result on epoch 0: Acc: 10.000%\n",
            "Test result on epoch 1: Acc: 10.010%\n",
            "Test result on epoch 2: Acc: 23.170%\n",
            "Test result on epoch 3: Acc: 38.000%\n",
            "Test result on epoch 4: Acc: 49.970%\n",
            "Test result on epoch 5: Acc: 61.900%\n",
            "Test result on epoch 6: Acc: 70.330%\n",
            "Test result on epoch 7: Acc: 73.950%\n",
            "Finished Training after 899.6727440357208 s \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULBxdNLti10X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dadbbe82-16a1-447b-876d-9e2b68f39304"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test result on epoch 7: Acc: 30.720%\n",
            "Finished Training after 678.0140495300293 s \n"
          ]
        }
      ],
      "source": [
        "## CODE REFERENCED FROM ECE570 ASSIGNMENT 3 ##\n",
        "for i in rank:\n",
        "  start = time.time()\n",
        "  resnet18_FREEZE_LORA =  models.resnet18(pretrained=True)\n",
        "  resnet18_FREEZE_LORA.to(device)\n",
        "  for param in resnet18_FREEZE_LORA.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  for param in resnet18_FREEZE_LORA.layer3.parameters():\n",
        "      param.requires_grad = True\n",
        "  for param in resnet18_FREEZE_LORA.layer4.parameters():\n",
        "      param.requires_grad = True\n",
        "  resnet18_FREEZE_LORA.fc = LoraLinear(layer=resnet18_FREEZE_LORA.fc, rank=i, alpha=i, dropout=0.5, merge_weights=False)\n",
        "  optimizer = optim.SGD([param for param in resnet18_FREEZE_LORA.parameters() if param.requires_grad], lr=0.1, momentum=0.8)\n",
        "  for epoch in range(max_epoch):\n",
        "    train(resnet18_FREEZE_LORA, criterion, optimizer, train_loader, epoch)\n",
        "  test(resnet18_FREEZE_LORA, criterion, test_loader, epoch)\n",
        "  end = time.time()\n",
        "  print(f'Finished Training after {end-start} s ')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_conv_with_lora(model, rank, alpha, dropout, merge_weights):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            lora_layer = LoraConv(module, rank, alpha, dropout, merge_weights)\n",
        "            setattr(model, name, lora_layer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "dluPNW_d5Z0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CODE REFERENCED FROM ECE570 ASSIGNMENT 3 ##\n",
        "for i in rank:\n",
        "  start = time.time()\n",
        "  resnet18_CONV_LORA =  models.resnet18(pretrained=True)\n",
        "  resnet18_CONV_LORA.to(device)\n",
        "\n",
        "  replace_conv_with_lora(resnet18_CONV_LORA, i, i, 0.1, False)\n",
        "  resnet18_CONV_LORA.fc = LoraLinear(layer=resnet18_CONV_LORA.fc, rank=i, alpha=i, dropout=0.5, merge_weights=False)\n",
        "  optimizer = optim.SGD([param for param in resnet18_CONV_LORA.parameters() if param.requires_grad], lr=0.1, momentum=0.8)\n",
        "  for epoch in range(max_epoch):\n",
        "    train(resnet18_CONV_LORA, criterion, optimizer, train_loader, epoch)\n",
        "  test(resnet18_CONV_LORA, criterion, test_loader, epoch)\n",
        "  end = time.time()\n",
        "  print(f'Finished Training after {end-start} s ')"
      ],
      "metadata": {
        "id": "6nNaImRM5Ady"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kf0YTdHHlniN"
      },
      "outputs": [],
      "source": [
        "## CODE TAKEN FROM ECE570 ASSIGNMENT 3 ##\n",
        "\n",
        "resnet18_FT = models.resnet18(pretrained=True)\n",
        "resnet18_FT = resnet18_FT.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlJ6-yoD3EED"
      },
      "outputs": [],
      "source": [
        "## CODE TAKEN FROM ECE570 ASSIGNMENT 3 ##\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "max_epoch = 8\n",
        "optimizer = optim.SGD(resnet18_FT.parameters(), lr=0.1, momentum=0.8)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for epoch in range(max_epoch):\n",
        "  train(resnet18_FT, criterion, optimizer, train_loader, epoch)\n",
        "test(resnet18_FT, criterion, test_loader, epoch)\n",
        "\n",
        "end = time.time()\n",
        "print(f'Finished Training after {end-start} s ')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18_FR = models.resnet18(pretrained=True)\n",
        "resnet18_FR = resnet18_FR.to(device)"
      ],
      "metadata": {
        "id": "pxw-gVbP8k1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "for param in resnet18_FR.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in resnet18_FR.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in resnet18_FREEZE_LORA.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in resnet18_FREEZE_LORA.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "max_epoch = 8\n",
        "optimizer = optim.SGD([param for param in resnet18_FR.parameters() if param.requires_grad], lr=0.1, momentum=0.8)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "for epoch in range(max_epoch):\n",
        "  train(resnet18_FR, criterion, optimizer, train_loader, epoch)\n",
        "test(resnet18_FR, criterion, test_loader, epoch)\n",
        "\n",
        "end = time.time()\n",
        "print(f'Finished Training after {end-start} s ')"
      ],
      "metadata": {
        "id": "KGoh3rdrovzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in rank:\n",
        "  start = time.time()\n",
        "  resnet18_TE = models.resnet18(pretrained=True)\n",
        "  resnet18_TE = resnet18_TE.to(device)\n",
        "\n",
        "  peft_config = LoraConfig(\n",
        "      task_type=\"IMAGE_CLASSIFICATION\",\n",
        "      r=i,\n",
        "      lora_alpha=i,\n",
        "      target_modules=[\"fc\"],\n",
        "      lora_dropout=0.5,)\n",
        "  resnet18_TE = get_peft_model(resnet18_TE, peft_config)\n",
        "  for param in resnet18_TE.layer3.parameters():\n",
        "      param.requires_grad = True\n",
        "  for param in resnet18_TE.layer4.parameters():\n",
        "      param.requires_grad = True\n",
        "\n",
        "  optimizer = optim.SGD(resnet18_TE.parameters(), lr=0.1, momentum=0.8)\n",
        "\n",
        "  for epoch in range(max_epoch):\n",
        "      train(resnet18_TE, criterion, optimizer, train_loader, epoch)\n",
        "  test(resnet18_TE, criterion, test_loader, epoch)\n",
        "  end = time.time()\n",
        "  print(f'Finished Training after {end - start} s ')"
      ],
      "metadata": {
        "id": "ib0hFZvIM6Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in rank:\n",
        "  start = time.time()\n",
        "  resnet18_TE = models.resnet18(pretrained=True)\n",
        "  resnet18_TE = resnet18_TE.to(device)\n",
        "\n",
        "  peft_config = LoraConfig(\n",
        "      task_type=\"IMAGE_CLASSIFICATION\",\n",
        "      r=i,\n",
        "      lora_alpha=i,\n",
        "      target_modules=[\"fc\", \"conv\"],\n",
        "      lora_dropout=0.5,)\n",
        "  resnet18_TE = get_peft_model(resnet18_TE, peft_config)\n",
        "\n",
        "  optimizer = optim.SGD(resnet18_TE.parameters(), lr=0.1, momentum=0.8)\n",
        "\n",
        "  for epoch in range(max_epoch):\n",
        "      train(resnet18_TE, criterion, optimizer, train_loader, epoch)\n",
        "  test(resnet18_TE, criterion, test_loader, epoch)\n",
        "  end = time.time()\n",
        "  print(f'Finished Training after {end - start} s ')"
      ],
      "metadata": {
        "id": "-9Q-d30V57W1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyMpn7uO4i8gIrhiHiwEYgAd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}